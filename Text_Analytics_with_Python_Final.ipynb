{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fVv22izm1Ie"
      },
      "source": [
        "Hello everyone. My name is Sean Ru. I am a third-year CS major here at the Data Visualization Lab.\n",
        "\n",
        "For today's workshop, we will be using the NLTK library to walk you through some basic steps of a text analytics project. To be more specific, we will be talking word/term frequency in texts. For those of you who don't know, NLTK is a library used to work with human language data.\n",
        "\n",
        "For those of you who have heard of text analytics before, you guys might also think of text mining and text analysis, which are not the same as text analytics.\n",
        "\n",
        "According to a blog called monkeylearn, “Text analytics is the automated process of translating large volumes of unstructured text into quantitative data to uncover insights, trends, and patterns. Combined with data visualization tools, this technique enables companies to understand the story behind the numbers and make better decisions.\" \n",
        "\n",
        "On the other hand, text mining and text analysis are the same thing. It’s the process of transforming unstructured text into structured data for easy analysis through utilizing Natural Language Processing, which is the process of training machines/programs to understand the human language and process it. \n",
        "\n",
        "So you might be thinking, well that sounds really similar to text analytics. Well the difference between text mining and text analytics is actually that, text mining delivers qualitative results and text analytics delivers quantitative results.\n",
        "\n",
        "\n",
        "NLTK is one of the most popular tools to process human language data. \n",
        "\n",
        "Some basic steps of text analysis we are going to demonstrate include:\n",
        "\n",
        "       -tokenize text\n",
        "       -clean punctuations \n",
        "       -remove stop words \n",
        "       -stemm words \n",
        "       -tag words \n",
        "       -vocabulary diversity\n",
        "       -word frequency distribution\n",
        "We actually are not going to go over sentiment analysis, sorry about that, sentiment analysis is actually text mining/analysis. So I'll have to fix that. So, let's start with some of the majory libraries we will have to use to import the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMo9jMjzm1Ih"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path #provides an object api for working with files and directories\n",
        "import pandas as pd #library used for data science and machine learning\n",
        "import os #provides functions for interacting with operating systems\n",
        "import glob #used to return all file paths that match a specific pattern\n",
        "import sys #provides functions and variables used to manipulate different part of the Python runtime environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSxMor5vm1Ii"
      },
      "source": [
        "### Import files\n",
        "\n",
        "Make sure to import Sample_data into Google Colab in files.\n",
        "\n",
        "In order to import a folder of files, we use the os.chdir function to first navigate to the right directory.\n",
        "\n",
        "Then we use glob.glob function to iterate through all files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ngBGcyMm1Ii"
      },
      "outputs": [],
      "source": [
        "my_dir = \"Sample_data\"\n",
        "os.chdir(my_dir)   #change the current working directory to specified path. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviewList=[]\n",
        "#code through here\n",
        "for files in glob.glob(\"*.txt\"):   #glob.glob returns a list of pathnames. It helps us loop through all files that are .txt in the sample folder\n",
        "    df = pd.read_csv(files) #dataframe, data structure that organizes data into a 2-dimensional table of rows and columns, like a spreadsheet\n",
        "    #print(df)\n",
        "    for content in df:  \n",
        "        reviewList.append(content) #add all the data (or in this case the strings in the files in sample data) to this list\n",
        "print (reviewList) #see the list of Strings from all the .txt files"
      ],
      "metadata": {
        "id": "450u00VYaz-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsaj5MSKm1Ij"
      },
      "source": [
        "Convert the review list into a huge string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "EJL8CQZ1m1Ij"
      },
      "outputs": [],
      "source": [
        "str1 = \" \" #String that will combine all the strings in the reviewList into 1 huge string, want it as a bag of words\n",
        "data = str1.join(reviewList) #combines all the strings, data is a string\n",
        "#allows us to not have to use anymore loops to do the same function for all the separate strings in the reviewList\n",
        "data = data.replace(\"<br />\",\"\") #deletes any breaks or \\n\n",
        "print (data)\n",
        "\n",
        "# tokens = nltk.word_tokenize(str(words)) #convert words from list to string and tokenize them\n",
        "# print (tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V57_YF5m1Ik"
      },
      "source": [
        "### Remove punctuation and stop words\n",
        "Tokenization is the process by which big quantity of text is divided into smaller parts called tokens.\n",
        "\n",
        "Stop words are words that are so commonly used that they carry very little useful information.\n",
        "\n",
        "http://www.nltk.org/nltk_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2tvpESTm1Ik"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# nltk.download_shell() for mac users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAMtYkTZm1Il"
      },
      "outputs": [],
      "source": [
        "# import re, pprint\n",
        "from nltk.corpus import stopwords #stopwords are words that can be safely ignored, they don't add much meaning to a sentence outside of grammar\n",
        "from nltk.tokenize import word_tokenize #word is splits a string into individual words called tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgQpOXZxm1Il"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "punctuations = \"$?:!.,;/\\\\&*)--(...-``''\" #define\n",
        "\n",
        "#tokens = word_tokenize(data) #convert words from list to string and tokenize them\n",
        "tokens = word_tokenize(data.replace(\".\",\" \")) #fill the periods with spaces.\n",
        "words = [word.lower() for word in tokens if not word in punctuations] #lowercase everything and if it isn't a punctuation add it to words\n",
        "print(sorted(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqDfBOaum1Im"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) #tell it we want english\n",
        "\n",
        "## Add extra stop words after viewing the results\n",
        "# stop_words.add(\"m\")\n",
        "# stop_words.add(\"'s\")\n",
        "# stop_words.remove(\"yourself\")\n",
        "# print (sorted(stop_words))\n",
        "\n",
        "filtered_words = [word for word in words if not word in stop_words] #if not a stop word, add to filtered_words\n",
        "print(sorted(filtered_words)) #sort it aphabetically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mbhM-Hzm1In"
      },
      "source": [
        "### Stemming and Lemmatization\n",
        "\n",
        "To reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
        "\n",
        "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
        "\n",
        "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5CfmINPm1In"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "#process for removing the commoner morphological and inflexional endings from words in English, liked,likes->like\n",
        "#simplify to the common base word\n",
        "from nltk.stem import SnowballStemmer\n",
        "#improved PorterStemmmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmer2 = SnowballStemmer(\"english\") #tell SnowballStemmer the language is English\n",
        "\n",
        "stem_words = [stemmer2.stem(words) for words in filtered_words]\n",
        "print (sorted(stem_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMSUP7RDm1Io"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet') #lexical database for English (nouns, adjectives, adverbs, verbs)\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "#the process of grouping together the different inflected forms of a word so they can be analyzed as a single item\n",
        "wnl = WordNetLemmatizer()\n",
        "print(wnl.lemmatize(\"cats\"))\n",
        "print(wnl.lemmatize(\"giggling\"))\n",
        "print(wnl.lemmatize(\"giggling\", \"v\"))\n",
        "\n",
        "lemm_words = [wnl.lemmatize(word) for word in filtered_words]\n",
        "print(sorted(lemm_words))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq22eaVNm1Io"
      },
      "source": [
        "### Speech Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfYcsQuVm1Io"
      },
      "source": [
        "    CC   coordinating conjunction\n",
        "    CD   cardinal digit\n",
        "    DT   determiner\n",
        "    EX   existential there (like: \"there is\" ... think of it like \"there exists\")\n",
        "    FW   foreign word\n",
        "    IN   preposition/subordinating conjunction\n",
        "    JJ   adjective 'big'\n",
        "    JJR   adjective, comparative 'bigger'\n",
        "    JJS   adjective, superlative 'biggest'\n",
        "    LS   list marker 1)\n",
        "    MD   modal could, will\n",
        "    NN   noun, singular 'desk'\n",
        "    NNS   noun plural 'desks'\n",
        "    NNP   proper noun, singular 'Harrison'\n",
        "    NNPS   proper noun, plural 'Americans'\n",
        "    PDT   predeterminer 'all the kids'\n",
        "    POS   possessive ending parent's\n",
        "    PRP   personal pronoun I, he, she\n",
        "    PRP$   possessive pronoun my, his, hers\n",
        "    \n",
        "    RB   adverb very, silently\n",
        "    RBR   adverb, comparative better\n",
        "    UH   interjection errrrrrrrm\n",
        "    VB   verb, base form take\n",
        "    VBD   verb, past tense took\n",
        "    VBG   verb, gerund/present participle taking\n",
        "    VBN   verb, past participle taken\n",
        "    VBP   verb, sing. present, non-3d take\n",
        "    VBZ   verb, 3rd person sing. present takes\n",
        "    WDT   wh-determiner which\n",
        "    WP   wh-pronoun who, what\n",
        "    WP$   possessive wh-pronoun whose\n",
        "    WRB   wh-abverb where, when\n",
        "    RBS   adverb, superlative best\n",
        "    RP   particle give up\n",
        "    TO   to go 'to' the store\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3UAyId0m1Ip"
      },
      "source": [
        "#### 1. Use tags for lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIPHNY7Lm1Ip"
      },
      "source": [
        "pos_tag gets the tag for the word, it comes in form of a list of tuples[(word1, tag1)(word2, tag2)(word3, tag3)].\n",
        "\n",
        "Use indexing to drill down: the first[0] gets to the individual tuples, the [1] gets to the tags, and the [0] grabs the first letter of a tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "XsCQaU0vm1Ip"
      },
      "outputs": [],
      "source": [
        "#tuples are collections of objects in order\n",
        "from nltk.corpus import wordnet  \n",
        "nltk.download('averaged_perceptron_tagger') #used for tagging words with their parts of speech (POS)\n",
        "\n",
        "#WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs \n",
        "#are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. \n",
        "#reminder, we removed stop words, so tags like determiners that included 'a', 'an', and 'the' will not be used as often\n",
        "#you'll see that the count for determiners, DT isn't 0, and that's becuz either we didn't actually remove all the stop words\n",
        "#through our code, or the pretrained model may have mistagged them. that's fine becuz this is an intro workshop and\n",
        "#even tho there is some error, we still largely get the job done\n",
        "\n",
        "#so by mistagging....\n",
        "print(nltk.pos_tag([\"meaningful\"])[0][1][0])\n",
        "#pretrained models can have errors\n",
        "\n",
        "#We are tagging each word so we can lemmatize it to common base form\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "#print (get_wordnet_pos(\"facing\"))\n",
        "#print (get_wordnet_pos(\"kindly\"))\n",
        "\n",
        "lemm_words = [wnl.lemmatize(w, get_wordnet_pos(w)) for w in filtered_words]\n",
        "print (sorted(lemm_words)[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlbu-C_Qm1Iq"
      },
      "source": [
        "#### 2. Tagging our data\n",
        "\n",
        "nltk.pos_tag() returns a tuple with the POS tag. The key here is to map NLTK’s POS tags to the format wordnet lemmatizer would accept. The get_wordnet_pos() function defined below does this mapping job.\n",
        "\n",
        "Reference: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbdJvCs9m1Iq"
      },
      "outputs": [],
      "source": [
        "text = word_tokenize(\"faking a review for tagging purpose\")\n",
        "nltk.pos_tag(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A98p2PVCm1Iq"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "#count hashable objects\n",
        "\n",
        "tags = nltk.pos_tag(lemm_words)\n",
        "#print (tags[:5])\n",
        "\n",
        "tag_counts = Counter(tag for word,tag in tags)\n",
        "print (tag_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfX9Mqqem1Ir"
      },
      "source": [
        "### Counting Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arzyob5Cm1Ir"
      },
      "source": [
        "#### 1. Check Vocabulary Diversity\n",
        "set() creates a distinct collection of the iterable elements (all words here)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIXlmiHim1Ir"
      },
      "outputs": [],
      "source": [
        "print (len(set(lemm_words)))  # number of distinct words\n",
        "print (len(lemm_words))    # number of total words\n",
        "print (\"The vocabulary diversity of the reviews is: \"+ str(len(set(lemm_words))/len(lemm_words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UCWRRMZm1Ir"
      },
      "source": [
        "#### 2. Count total words and unique words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdf31OEzm1Is"
      },
      "outputs": [],
      "source": [
        "# create a dictionary to store uniques words and their counts.\n",
        "count = {}\n",
        "for w in lemm_words:\n",
        "    if w in count:\n",
        "        count[w] += 1\n",
        "    else:\n",
        "        count[w] = 1\n",
        "    \n",
        "count = {k: v for k, v in sorted(count.items(), key=lambda item: item[1], reverse=True)} \n",
        "# count has two items: key and value\n",
        "# word is the key and count is the value\n",
        "# we are sorting from greatest to smallest\n",
        "# \"key=lambda\" allows us to sort our dictionary by value. \n",
        "# This is an example of a Lambda function, which is a function without a name.\n",
        "# default it's ascending sort, \"reverse=True\" flips the order to descending.\n",
        "\n",
        "print (count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5OyoKi4m1Is"
      },
      "source": [
        "#### 3. NLTK's Frequency Distributions Functions\n",
        "\n",
        "    fdist = FreqDist(samples)\tcreate a frequency distribution containing the given samples\n",
        "    fdist[sample] += 1\tincrement the count for this sample\n",
        "    fdist['monstrous']\tcount of the number of times a given sample occurred\n",
        "    fdist.freq('monstrous')\tfrequency of a given sample\n",
        "    fdist.N()\ttotal number of samples\n",
        "    fdist.most_common(n)\tthe n most common samples and their frequencies\n",
        "    for sample in fdist:\titerate over the samples\n",
        "    fdist.max()\tsample with the greatest count\n",
        "    fdist.tabulate()\ttabulate the frequency distribution\n",
        "    fdist.plot()\tgraphical plot of the frequency distribution\n",
        "    fdist.plot(cumulative=True)\tcumulative plot of the frequency distribution\n",
        "    fdist1 |= fdist2\tupdate fdist1 with counts from fdist2\n",
        "    fdist1 < fdist2\ttest if samples in fdist1 occur less frequently than in fdist2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIMrS_BKm1Is"
      },
      "outputs": [],
      "source": [
        "from nltk import FreqDist #frequency distribution\n",
        "\n",
        "freq_words=FreqDist(lemm_words)\n",
        "freq_words.most_common(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoG0TKxCm1Is"
      },
      "outputs": [],
      "source": [
        "! pip install matplotlib #creating static, animated, and interactive visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7CsFXMNm1Is"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt #basically matlab\n",
        "#plt.figure(figsize=(12, 5))  #(x,y)\n",
        "#plt.title(\"Cummulative Frequency Distribution\")\n",
        "plot1 = FreqDist(lemm_words).plot(30, cumulative=True, color=\"black\") #frequency distribution\n",
        "\n",
        "plt.figure(figsize=(12, 5))  \n",
        "plt.title(\"Non-cummulative Frequency Distribution\")\n",
        "plot2 = FreqDist(lemm_words).plot(30, cumulative=False, color=\"purple\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HMkHVEOm1It"
      },
      "source": [
        "### Dispersion Plot\n",
        "\n",
        "Show the location of words in the collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP8yETx8m1It"
      },
      "outputs": [],
      "source": [
        "from nltk.draw.dispersion import dispersion_plot \n",
        "#allows for visualization of the lexical dispersion of words in a corpus, which is a collection of texts\n",
        "dispersion_plot(lemm_words, ['movie', 'scene','character'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuw0eaBHm1It"
      },
      "source": [
        "Reference: \n",
        "\n",
        "        https://www.nltk.org/book/ch01.html\n",
        "        https://www.nltk.org/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}